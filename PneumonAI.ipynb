{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7953c10c-0dbc-47e6-8abf-f60c924bc262",
   "metadata": {},
   "source": [
    "# PneumonAI\n",
    "This notebook shares functions used to create and evaluate PneumonAI. This AI is tasked to classify Chest X-Ray in two categories : PNEUMONIA or NORMAL and was trained on the [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia). A secondary dataset has been used for experiments: [NIH Chest X-rays](https://www.kaggle.com/datasets/nih-chest-xrays/data). Both datasets are available on Kaggle.\n",
    "## Author\n",
    "PneumonAI was created as Laureline Willem's master's thesis. The project sets to offer an Open Source interpretable tool for pneumonia detection.\n",
    "## License\n",
    "The following work is made available under MIT license.\n",
    "\n",
    "Copyright Â© 2024, Laureline Willem\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd53a72-72d8-4bda-9bb1-e36acd40f674",
   "metadata": {},
   "source": [
    "## Import packages and loading the datasets\n",
    "Note that the preprocessing has been reworked beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Loads and returns child & adult datasets\n",
    "# Input : \n",
    "#   - path : path to primary dataset (pediatric X-rays)\n",
    "#   - pathsecondary : path to secondary dataset (adult X-rays)\n",
    "# Output : raw sets in the following order : raining, validation and testing sets.\n",
    "def load_both_datasets(path, pathsecondary) :\n",
    "    # Fix sizes\n",
    "    BATCH_SIZE = 32\n",
    "    IMG_SIZE = (512, 512)\n",
    "\n",
    "    # Child dataset\n",
    "    train = keras.utils.image_dataset_from_directory(path+\"/train\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    val = keras.utils.image_dataset_from_directory(path+\"/val\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    test = keras.utils.image_dataset_from_directory(path+\"/test\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    \n",
    "    # Adult dataset\n",
    "    trainadult = keras.utils.image_dataset_from_directory(pathsecondary+\"/train\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    valadult = keras.utils.image_dataset_from_directory(pathsecondary+\"/val\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    testadult = keras.utils.image_dataset_from_directory(pathsecondary+\"/test\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "\n",
    "    return train, val, test, trainadult, valadult, testadult\n",
    "\n",
    "# Returns the dataset preprocessed with data augmentation applied on the training set\n",
    "# Input : train, val and test : unpreprocessed tf.data.Dataset containing respectively the training, validation and testing sets.\n",
    "# Output : preprocessed sets in the following order : raining, validation and testing sets.\n",
    "def preprocessing_and_data_augmentation(train, val, test) :\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    seed = (1, 2)\n",
    "\n",
    "    # Preprocess and apply data augmenation to train set\n",
    "    train_dataset = train.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(lambda x, label:(tf.image.stateless_random_contrast(x, 0.2, 0.5, seed), label), num_parallel_calls = AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(lambda x, label:(tf.image.stateless_random_saturation(x, 0.5, 1.0, seed), label), num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "    # Preprocess validation and testing\n",
    "    validation_dataset = val.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "    test_dataset = test.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "    # Prefectching allows for better performance\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0476e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test, trainadult, valadult, testadult = load_both_datasets()\n",
    "train_dataset, val_dataset, test_dataset = preprocessing_and_data_augmentation(train, val, test)\n",
    "trainadult_dataset, valadult_dataset, testadult_dataset = preprocessing_and_data_augmentation(trainadult, valadult, testadult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b05e3-77f7-4aea-bf96-d4cdfcd759f3",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e22acce-e5f1-4ce2-88f6-ac32d2dcc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and returns a model using transfer learning from the argument base_model.\n",
    "# Input : base_model, a pretrained model from keras.applications to use as backbone\n",
    "# Output : the new model with backbone parameters frozen\n",
    "def create_model(base_model) :\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.outputs[0]\n",
    "    base_model.trainable=False\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    prediction_layer = tf.keras.layers.Dense(1)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    base_learning_rate = 0.0001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dd9f6-86e8-4750-9ff3-b86c5a99a967",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e0004d-58de-476c-af00-049a7966850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model\n",
    "# Inputs :\n",
    "#   - model : the model to train\n",
    "#   - train_dataset : the training set\n",
    "#   - validation_dataset : the validation set\n",
    "#   - initial_epochs : the maximal number of epochs\n",
    "# Output : history of the training\n",
    "def train_model(model, train_dataset, validation_dataset, initial_epochs = 100) :\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=3, verbose=False)\n",
    "    history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset, callbacks=[callback], verbose=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02e0e4-ada6-434f-8399-0a04c3dff997",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220b6bf4-2316-413a-b711-ac403e94475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the backbone for fine tuning.\n",
    "# Input : base_model : the backbone.\n",
    "def unfreeze_model(base_model) :\n",
    "    base_model.trainable=True\n",
    "\n",
    "# Trains the model\n",
    "# Inputs :\n",
    "#   - model : the model to fine tune\n",
    "#   - train_dataset : the training set\n",
    "#   - validation_dataset : the validation set\n",
    "#   - initial_epochs : the maximal number of epochs\n",
    "# Output : history of the training\n",
    "def fine_tune_model(model, train_dataset, validation_dataset, max_epochs = 100, min_lr=0.000001) :\n",
    "    unfreeze_model(base_model)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=10, verbose=False)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=min_lr)\n",
    "    history = model.fit(train_dataset,\n",
    "                    epochs=max_epochs,\n",
    "                    validation_data=validation_dataset, callbacks=[callback, reduce_lr], verbose=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058250d-6516-444f-ad68-6f0a46a31102",
   "metadata": {},
   "source": [
    "## GradCam\n",
    "From [Grad-CAM class activation visualization](https://keras.io/examples/vision/grad_cam/), with a few modifications to fit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17b3e4-7845-4be2-9922-793c0ca57df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Create a sub-model that outputs the feature maps and final prediction\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Use GradientTape to record gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "\n",
    "        # If pred_index is not specified, use the predicted class index\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # Calculate gradients\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Compute the heatmap\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalise the heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib as mpl\n",
    "def display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.001):\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = mpl.colormaps[\"jet\"]\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = img + jet_heatmap * alpha \n",
    "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a60375-9527-4b81-b7bd-8152b01fb952",
   "metadata": {},
   "source": [
    "### Extract Bounding Boxes\n",
    "We accept having multiple boxes, as pneumonias can be multifocal (i.e. be visible in multiple unrelated areas of the chest). The algorithm is fairly naive and clusters together neighboring positive pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b298e-8806-41b2-84cc-27e4d8d08d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bounding Boxes from the GradCAM Heatmap\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap\n",
    "# Output : a set containing coordinates from the top left and bottom right corners of the bounding boxe(s)\n",
    "def ExtractBB(heatmap, threshold=0.5) :\n",
    "    # Binarize the set\n",
    "    binarization = 1*(heatmap>=threshold)\n",
    "    positive = set()\n",
    "    for i in range(len(binarization)) :\n",
    "        for j in range(len(binarization[i])) :\n",
    "            if binarization[i][j] == 1 :\n",
    "                positive.add((i, j))\n",
    "    # cluster zones together\n",
    "    clusters = set()\n",
    "    while len(positive)>0 :\n",
    "        cluster = set()\n",
    "        # Get a coordinate\n",
    "        coord = list(positive)[0]\n",
    "        # Add the initial pixel to the new cluster\n",
    "        cluster.add(coord)\n",
    "        # Remove it from positive to avoid treating it twice\n",
    "        positive.remove(coord)\n",
    "        # Start Growing the region\n",
    "        get_next_to(heatmap, positive, coord, cluster)\n",
    "        # Create the bounding box containing all the needed pixels\n",
    "        mini = len(heatmap)\n",
    "        minj = len(heatmap[0])\n",
    "        maxi = 0\n",
    "        maxj = 0\n",
    "        for elem in cluster :\n",
    "            mini = min(mini, elem[0])\n",
    "            minj = min(minj, elem[1])\n",
    "            maxi = max(maxi, elem[0])\n",
    "            maxj = max(maxj, elem[1])\n",
    "        # resize bounding box from heatmap size (16x16) to image size (512x512)\n",
    "        clusters.append([mini*32, minj*32, maxi*32+32, maxj*32+32])\n",
    "        # add the cluster to the set\n",
    "        clusters.add((mini, minj, maxi, maxj))\n",
    "    return clusters\n",
    "\n",
    "# Checks Neighbooring pixels recursively until the cluster is bordered by negative pixels. Updated positive and cluster in the process.\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap\n",
    "#   - positive : set of unchecked coordinates of positive pixels\n",
    "#   - coord : Coordinates whose neightboor must be checked\n",
    "#   - cluster : Set containing coordinates of pixels in the cluster\n",
    "# Output : /\n",
    "def get_next_to(heatmap, positive, coord, cluster) :\n",
    "    if coord[0] > 0 :\n",
    "        newcoord = (coord[0]-1, coord[1])\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] > 0 :\n",
    "            newcoord = (coord[0]-1, coord[1]-1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "            newcoord = (coord[0]-1, coord[1]+1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[0] < len(heatmap)-1 :\n",
    "        newcoord = (coord[0]+1, coord[1])\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] > 0 :\n",
    "            newcoord = (coord[0]+1, coord[1]-1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "            newcoord = (coord[0]+1, coord[1]+1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[1] > 0 :\n",
    "        newcoord = (coord[0], coord[1]-1)\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "        newcoord = (coord[0], coord[1]+1)\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "\n",
    "# Updates positive and clusters if newcoords contains the coordinates of a positive pixel.\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap.\n",
    "#   - positive : set of unchecked coordinates of positive pixels.\n",
    "#   - coord : Coordinates whose neightboor must be checked.\n",
    "#   - cluster : Set containing coordinates of pixels in the cluster.\n",
    "# Output : /\n",
    "def update(heatmap, positive, newcoord, cluster) :\n",
    "    if newcoord in positive :\n",
    "        # Add positive neighboor to the cluster\n",
    "        cluster.add(newcoord)\n",
    "        # Remove it from positive to avoid treating it twice\n",
    "        positive.remove(newcoord)\n",
    "        # Check its own neighboor\n",
    "        get_next_to(heatmap, positive, newcoord, cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1dcaf-eac0-4ee7-9665-c2262195e49d",
   "metadata": {},
   "source": [
    "## Compute IoU (Intersection over Union)\n",
    "From [Intersection over Union (IoU) for object detection](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/), adapted for multiple bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511a586-0501-4292-831f-c82968510574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes IoU between boxesA and boxesB.\n",
    "# Inputs :\n",
    "#   - boxesA : first set of boxes.\n",
    "#   - boxesB : second set of boxes.\n",
    "# Output : IoU between boxesA and boxesB.\n",
    "def bb_intersection_over_union(boxesA, boxesB):\n",
    "    interArea = 0\n",
    "    boxesAArea = 0\n",
    "    boxesBArea = 0\n",
    "    for boxA in boxesA :\n",
    "        boxesAArea += (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "        for boxB in boxesB :\n",
    "            # determine the (x, y)-coordinates of the intersection rectangle\n",
    "            xA = max(boxA[0], boxB[0])\n",
    "            yA = max(boxA[1], boxB[1])\n",
    "            xB = min(boxA[2], boxB[2])\n",
    "            yB = min(boxA[3], boxB[3])\n",
    "            # compute the area of intersection rectangle\n",
    "            interArea += max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "            # compute the area of both the prediction and ground-truth\n",
    "            # rectangles\n",
    "    for boxB in boxesB :\n",
    "        boxesBArea += (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxesAArea + boxesBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25194c66",
   "metadata": {},
   "source": [
    "IoU computation between annotations and Grad-CAM result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tf_image.core.bboxes.resize import resize\n",
    "# Computes IoU between given annotations and Grad-CAM results for all images given at path.\n",
    "# Inputs :\n",
    "#   - path : path to the csv containing annotations.\n",
    "#   - threshold : threshold for gradcam heatmap binarisation.\n",
    "# Output : an array with an entry for each image reporting IoU between the annotations and bounding box extracted with Grad-CAM.\n",
    "def IoUOverAnnotation(path,threshold=0.5) :\n",
    "    df = pd.read_csv(path)\n",
    "    IoU = []\n",
    "    # Iterate on all the patient analysed\n",
    "    while df.shape[0] > 0 :\n",
    "        imgname = df.iloc[0][\"image_name\"]\n",
    "        # Get all boxes for this image\n",
    "        allimgbox = df.loc[df['image_name'] == imgname]\n",
    "        df = df.loc[df['image_name'] != imgname]\n",
    "        img = tf.image.decode_image(tf.io.read_file('chest_xray/test/PNEUMONIA/' + imgname), channels=3)\n",
    "        boxes = []\n",
    "        # format boxes\n",
    "        while allimgbox.shape[0] > 0  :\n",
    "            bbox = allimgbox.iloc[0]\n",
    "            allimgbox = allimgbox.loc[allimgbox['bbox_y'] != bbox['bbox_y']]\n",
    "            miny = bbox[\"bbox_y\"]\n",
    "            minx = bbox[\"bbox_x\"]\n",
    "            maxy = bbox[\"bbox_y\"] + bbox[\"bbox_width\"]\n",
    "            maxx = bbox[\"bbox_x\"] + bbox[\"bbox_height\"]\n",
    "            boxes.append([miny, minx, maxy, maxx])\n",
    "        # Apply equivalent preprocessing to annotated boxes\n",
    "        boxes = tf.constant(boxes)\n",
    "        img, boxes = resize(img, boxes, 512, 512, keep_aspect_ratio=True, random_method=False)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        boxes = boxes.numpy()\n",
    "        boxesInOrder = []\n",
    "        # tf_image declares y before x, while we to x before y, so we switch the elements.\n",
    "        for i in boxes :\n",
    "            boxesInOrder.append([i[1], i[0], i[3], i[2]])\n",
    "        boxes = boxesInOrder\n",
    "        # Compute Grad-CAM heatmap and extract bounding boxes\n",
    "        heatmap = make_gradcam_heatmap(np.expand_dims(img, axis=0), model, \"Conv_1\")\n",
    "        gradcamBB = ExtractBB(heatmap,threshold=threshold)\n",
    "        # Compute IoU and append it to all computed IoU\n",
    "        IoU.append(bb_intersection_over_union(boxes, gradcamBB))\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd2187",
   "metadata": {},
   "source": [
    "IoU between annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Computes IoU between given annotations for all common images between path1 and path2.\n",
    "# Inputs :\n",
    "#   - path1 : path to the first csv containing annotations.\n",
    "#   - path2 : path to the second csv containing annotations.\n",
    "# Output : an array with an entry for each image reporting IoU between the annotations.\n",
    "def IoUBetweenAnnotation(path1, path2) :\n",
    "    df1= pd.read_csv(path1)\n",
    "    df2= pd.read_csv(path2)\n",
    "    IoU = []\n",
    "    # Iterate on all the patient analysed\n",
    "    while df1.shape[0] > 0 :\n",
    "        imgname = df1.iloc[0][\"image_name\"]\n",
    "        # Get all boxes for this image\n",
    "        allimgbox1 = df1.loc[df1['image_name'] == imgname]\n",
    "        allimgbox2 = df2.loc[df2['image_name'] == imgname]\n",
    "        df1 = df1.loc[df1['image_name'] != imgname]\n",
    "        df2 = df2.loc[df2['image_name'] != imgname]\n",
    "        if allimgbox2.shape[0] > 0 :\n",
    "            boxes1 = []\n",
    "            # format boxes\n",
    "            while allimgbox1.shape[0] > 0  :\n",
    "                bbox1 = allimgbox1.iloc[0]\n",
    "                allimgbox1 = allimgbox1.loc[allimgbox1['bbox_y'] != bbox1['bbox_y']]\n",
    "                miny1 = bbox1[\"bbox_y\"]\n",
    "                minx1 = bbox1[\"bbox_x\"]\n",
    "                maxy1 = bbox1[\"bbox_y\"] + bbox1[\"bbox_width\"]\n",
    "                maxx1 = bbox1[\"bbox_x\"] + bbox1[\"bbox_height\"]\n",
    "                boxes1.append([miny1, minx1, maxy1, maxx1])\n",
    "            boxes2 = []\n",
    "            while allimgbox2.shape[0] > 0  :\n",
    "                bbox2 = allimgbox2.iloc[0]\n",
    "                allimgbox2 = allimgbox2.loc[allimgbox2['bbox_y'] != bbox2['bbox_y']]\n",
    "                miny2 = bbox2[\"bbox_y\"]\n",
    "                minx2 = bbox2[\"bbox_x\"]\n",
    "                maxy2 = bbox2[\"bbox_y\"] + bbox2[\"bbox_width\"]\n",
    "                maxx2 = bbox2[\"bbox_x\"] + bbox2[\"bbox_height\"]\n",
    "                boxes2.append([miny2, minx2, maxy2, maxx2])\n",
    "            boxes1 = tf.constant(boxes1)\n",
    "            boxes2 = tf.constant(boxes2)\n",
    "            boxes1 = boxes1.numpy()\n",
    "            boxes2 = boxes2.numpy()\n",
    "            boxesInOrder1 = []\n",
    "            # tf_image declares y before x, while we to x before y, so we switch the elements.\n",
    "            for i in boxes1 :\n",
    "                boxesInOrder1.append([i[1], i[0], i[3], i[2]])\n",
    "            boxesInOrder2 = []\n",
    "            # tf_image declares y before x, while we to x before y, so we switch the elements.\n",
    "            for i in boxes2 :\n",
    "                boxesInOrder2.append([i[1], i[0], i[3], i[2]])\n",
    "            boxes1 = boxesInOrder1\n",
    "            boxes2 = boxesInOrder2\n",
    "            # Compute IoU and append it to all computed IoU\n",
    "            IoU.append(bb_intersection_over_union(boxes1, boxes2))\n",
    "    return IoU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
