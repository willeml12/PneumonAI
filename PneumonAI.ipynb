{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7953c10c-0dbc-47e6-8abf-f60c924bc262",
   "metadata": {},
   "source": [
    "# PneumonAI\n",
    "This notebook implements and experiments with PneumonAI. This AI is tasked to classify Chest X-Ray in two categories : PNEUMONIA or Normal and was trained on the [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) and [NIH Chest X-rays](https://www.kaggle.com/datasets/nih-chest-xrays/data), both available on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd53a72-72d8-4bda-9bb1-e36acd40f674",
   "metadata": {},
   "source": [
    "## Import packages and loading the datasets\n",
    "Note that the preprocessing has been reworked beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25187f2b-ce20-4474-8caf-8e9fad39a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loads and returns child & adult datasets\n",
    "# Output : raw sets in the following order : raining, validation and testing sets.\n",
    "def load_both_datasets() :\n",
    "    # Fix sizes\n",
    "    BATCH_SIZE = 32\n",
    "    IMG_SIZE = (512, 512)\n",
    "\n",
    "    # Child dataset\n",
    "    train = keras.utils.image_dataset_from_directory(\"chest_xray/training1\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    val = keras.utils.image_dataset_from_directory(\"chest_xray/validation1\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    test = keras.utils.image_dataset_from_directory(\"chest_xray/test\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    \n",
    "    # Adult dataset\n",
    "    trainadult = keras.utils.image_dataset_from_directory(\"NIH Lungs/all/train\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    valadult = keras.utils.image_dataset_from_directory(\"NIH Lungs/all/val\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "    testadult = keras.utils.image_dataset_from_directory(\"NIH Lungs/all/test\", labels='inferred', color_mode = 'rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, pad_to_aspect_ratio = True)\n",
    "\n",
    "    return train, val, test, trainadult, valadult, testadult\n",
    "\n",
    "# Returns the dataset preprocessed with data augmentation applied on the training set\n",
    "# Input : train, val and test : unpreprocessed tf.data.Dataset containing respectively the training, validation and testing sets.\n",
    "# Output : preprocessed sets in the following order : raining, validation and testing sets.\n",
    "def preprocessing_and_data_augmentation(train, val, test) :\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    seed = (1, 2)\n",
    "\n",
    "    # Preprocess and apply data augmenation to train set\n",
    "    train_dataset = train.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(lambda x, label:(tf.image.stateless_random_contrast(x, 0.2, 0.5, seed), label), num_parallel_calls = AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(lambda x, label:(tf.image.stateless_random_saturation(x, 0.5, 1.0, seed), label), num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "    # Preprocess validation and testing\n",
    "    validation_dataset = val.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "    test_dataset = test.map(lambda img, label: (keras.applications.mobilenet_v2.preprocess_input(img), label), num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "    # Prefectching allows for better performance\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e276d9-5d57-4594-8c13-b01c0b35afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test, trainadult, valadult, testadult = load_both_datasets()\n",
    "train_dataset, val_dataset, test_dataset = preprocessing_and_data_augmentation(train, val, test)\n",
    "trainadult_dataset, valadult_dataset, testadult_dataset = preprocessing_and_data_augmentation(trainadult, valadult, testadult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b05e3-77f7-4aea-bf96-d4cdfcd759f3",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22acce-e5f1-4ce2-88f6-ac32d2dcc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and returns a model using transfer learning from the argument base_model.\n",
    "# Input : base_model, a pretrained model from keras.applications to use as backbone\n",
    "# Output : the new model with backbone parameters frozen\n",
    "def create_model(base_model) :\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.outputs[0]\n",
    "    base_model.trainable=False\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    prediction_layer = tf.keras.layers.Dense(1)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    base_learning_rate = 0.0001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dd9f6-86e8-4750-9ff3-b86c5a99a967",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0004d-58de-476c-af00-049a7966850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model\n",
    "# Inputs :\n",
    "#   - model : the model to train\n",
    "#   - train_dataset : the training set\n",
    "#   - validation_dataset : the validation set\n",
    "#   - initial_epochs : the maximal number of epochs\n",
    "# Output : history of the training\n",
    "def train_model(model, train_dataset, validation_dataset, initial_epochs = 100) :\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=3, verbose=False)\n",
    "    history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset, callbacks=[callback], verbose=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02e0e4-ada6-434f-8399-0a04c3dff997",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b6bf4-2316-413a-b711-ac403e94475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the backbone for fine tuning.\n",
    "# Input : base_model : the backbone.\n",
    "def unfreeze_model(base_model) :\n",
    "    base_model.trainable=True\n",
    "\n",
    "# Trains the model\n",
    "# Inputs :\n",
    "#   - model : the model to fine tune\n",
    "#   - train_dataset : the training set\n",
    "#   - validation_dataset : the validation set\n",
    "#   - initial_epochs : the maximal number of epochs\n",
    "# Output : history of the training\n",
    "def fine_tune_model(model, train_dataset, validation_dataset, max_epochs = 100, min_lr=0.000001) :\n",
    "    unfreeze_model(base_model)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=10, verbose=False)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=min_lr)\n",
    "    history = model.fit(train_dataset,\n",
    "                    epochs=max_epochs,\n",
    "                    validation_data=validation_dataset, callbacks=[callback, reduce_lr], verbose=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058250d-6516-444f-ad68-6f0a46a31102",
   "metadata": {},
   "source": [
    "## GradCam\n",
    "From [Grad-CAM class activation visualization](https://keras.io/examples/vision/grad_cam/), with a few modifications to fit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17b3e4-7845-4be2-9922-793c0ca57df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Create a sub-model that outputs the feature maps and final prediction\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Use GradientTape to record gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "\n",
    "        # If pred_index is not specified, use the predicted class index\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # Calculate gradients\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Compute the heatmap\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalise the heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib as mpl\n",
    "def display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.001):\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = mpl.colormaps[\"jet\"]\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = img + jet_heatmap * alpha \n",
    "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a60375-9527-4b81-b7bd-8152b01fb952",
   "metadata": {},
   "source": [
    "### Extract Bounding Boxes\n",
    "We accept having multiple boxes, as pneumonias can be multifocal (i.e. be visible in multiple unrelated areas of the chest). The algorithm is fairly naive and clusters together neighboring positive pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b298e-8806-41b2-84cc-27e4d8d08d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bounding Boxes from the GradCAM Heatmap\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap\n",
    "# Output : a set containing coordinates from the top left and bottom right corners of the bounding boxe(s)\n",
    "def ExtractBB(heatmap) :\n",
    "    # Binarize the set\n",
    "    binarization = 1*(heatmap>=0.5)\n",
    "    positive = set()\n",
    "    for i in range(len(binarization)) :\n",
    "        for j in range(len(binarization[i])) :\n",
    "            if binarization[i][j] == 1 :\n",
    "                positive.add((i, j))\n",
    "    # cluster zones together\n",
    "    clusters = set()\n",
    "    while len(positive)>0 :\n",
    "        cluster = set()\n",
    "        # Get a coordinate\n",
    "        coord = list(positive)[0]\n",
    "        # Add the initial pixel to the new cluster\n",
    "        cluster.add(coord)\n",
    "        # Remove it from positive to avoid treating it twice\n",
    "        positive.remove(coord)\n",
    "        # Start Growing the region\n",
    "        get_next_to(heatmap, positive, coord, cluster)\n",
    "        # Create the bounding box containing all the needed pixels\n",
    "        mini = len(heatmap)\n",
    "        minj = len(heatmap[0])\n",
    "        maxi = 0\n",
    "        maxj = 0\n",
    "        for elem in cluster :\n",
    "            mini = min(mini, elem[0])\n",
    "            minj = min(minj, elem[1])\n",
    "            maxi = max(maxi, elem[0])\n",
    "            maxj = max(maxj, elem[1])\n",
    "        # add the cluster to the set\n",
    "        clusters.add((mini, minj, maxi, maxj))\n",
    "    return clusters\n",
    "\n",
    "# Checks Neighbooring pixels recursively until the cluster is bordered by negative pixels. Updated positive and cluster in the process.\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap\n",
    "#   - positive : set of unchecked coordinates of positive pixels\n",
    "#   - coord : Coordinates whose neightboor must be checked\n",
    "#   - cluster : Set containing coordinates of pixels in the cluster\n",
    "# Output : /\n",
    "def get_next_to(heatmap, positive, coord, cluster) :\n",
    "    if coord[0] > 0 :\n",
    "        newcoord = (coord[0]-1, coord[1])\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] > 0 :\n",
    "            newcoord = (coord[0]-1, coord[1]-1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "            newcoord = (coord[0]-1, coord[1]+1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[0] < len(heatmap)-1 :\n",
    "        newcoord = (coord[0]+1, coord[1])\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] > 0 :\n",
    "            newcoord = (coord[0]+1, coord[1]-1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "        if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "            newcoord = (coord[0]+1, coord[1]+1)\n",
    "            update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[1] > 0 :\n",
    "        newcoord = (coord[0], coord[1]-1)\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "    if coord[1] < len(heatmap[coord[0]])-1 :\n",
    "        newcoord = (coord[0], coord[1]+1)\n",
    "        update(heatmap, positive, newcoord, cluster)\n",
    "\n",
    "# Updates positive and clusters if newcoords contains the coordinates of a positive pixel.\n",
    "# Inputs :\n",
    "#   - heatmap : GradCAM Heatmap\n",
    "#   - positive : set of unchecked coordinates of positive pixels\n",
    "#   - coord : Coordinates whose neightboor must be checked\n",
    "#   - cluster : Set containing coordinates of pixels in the cluster\n",
    "# Output : /\n",
    "def update(heatmap, positive, newcoord, cluster) :\n",
    "    if newcoord in positive :\n",
    "        # Add positive neighboor to the cluster\n",
    "        cluster.add(newcoord)\n",
    "        # Remove it from positive to avoid treating it twice\n",
    "        positive.remove(newcoord)\n",
    "        # Check its own neighboor\n",
    "        get_next_to(heatmap, positive, newcoord, cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1dcaf-eac0-4ee7-9665-c2262195e49d",
   "metadata": {},
   "source": [
    "## Compute IoU (Intersection over Union)\n",
    "From [Intersection over Union (IoU) for object detection](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/), adapted for multiple bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511a586-0501-4292-831f-c82968510574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the IoU of boxes present on the images\n",
    "# Inputs :\n",
    "#   - boxesA : boxes from the ground truth\n",
    "#   - boxesA : evaluated boxes\n",
    "# Output : IoU\n",
    "def bb_intersection_over_union(boxesA, boxesB):\n",
    "    i = 0\n",
    "    for boxA in boxesA :\n",
    "        for boxB in boxesB :\n",
    "        \t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "        \txA = max(boxA[0], boxB[0])\n",
    "        \tyA = max(boxA[1], boxB[1])\n",
    "        \txB = min(boxA[2], boxB[2])\n",
    "        \tyB = min(boxA[3], boxB[3])\n",
    "        \t# compute the area of intersection rectangle\n",
    "        \tinterArea += max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "        \t# compute the area of both the prediction and ground-truth\n",
    "        \t# rectangles, only on first iteration (once).\n",
    "            if i == 0 :\n",
    "            \tboxesAArea += (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "            \tboxesBArea += (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "                i += 1\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd48dac-1cfd-41bf-8c3a-336ed739b4c1",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Now that we implemented every tool we need, we can show the code used to build and experiments with PneumonAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32f286-ac8a-430b-8cdd-49173603631b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
